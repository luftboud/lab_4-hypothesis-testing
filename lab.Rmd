# Hypothesis testing

Work breakdown:

-   

-   Sofiia Trush - Problem 3

-   Iia Maharyta - Problem 4

The data for problems 1–3 are generated as follows: set $a_k := {k\ln (k^2n+ \pi)},\quad k≥1,$ where ${x}:= x−[x]$ is the fractional part of a number $x$ and $n$ is your id number. Sample realizations $X_1,...,X_{100}$ and $Y_1,...,Y_50$ from the hypothetical normal distributions $N (µ_1,σ^2_1 )$ and $N (µ_2,σ^2_2 )$ respectively are obtained as $x_k = Φ−1(a_k ), k= 1,...,100$, $y_l = Φ^{−1}(a_l+100), l= 1,...,50$, where $Φ$ is the cumulative distribution function of $N(0,1)$ and $Φ^{−1}$ is its inverse.

In R, you can define a function $f$ calculating ak from $k$, then apply $f$ to the whole list of $k$’s to get the list a.data of $a_k$ , and, finally get $x_k$ and $y_k$ by running qnorm on a.data.

**Instructions**: In problems 1–3, test $H_0$ vs $H_1$. To this end,

• point out what standard test you use and why;

• indicate the general form of the rejection region of the test $H_0$ vs $H_1$ of level 0.05;

• find out if $H_0$ should be rejected on the significance level 0.05;

• indicate the p-value of the test and comment whether you would reject $H_0$ for that value of $p$ and why

```{r}

id <- 5
set.seed(id)

frac_part <- function(x) {
  x - floor(x)
}

# a_k = { k * ln(k^2 * n + pi) }
a_fun <- function(k, n = id) {
  frac_part(k * log(k^2 * n + pi))
}

k_all   <- 1:150
a_data  <- sapply(k_all, a_fun)

# x_k = Phi^{-1}(a_k),  k = 1,...,100
# y_l = Phi^{-1}(a_{l+100}), l = 1,...,50
# Phi^{-1} = qnorm in R

xk <- qnorm(a_data[1:100])
yl <- qnorm(a_data[101:150])

#data_xy <- data.frame(
#  x = xk,
#  y = yl
#)

#head(data_xy)

```

## Problem 1

## Problem 2

## Problem 3
Using Kolmogorov-Smirnov test in R, check if:

(a) {$x_k$}$^{100}_{k=1}$ are normally distributed (with parameters calculated from the sample);

We want to check whether {$x_k$}$^{100}_{k=1}$ has normal distribution.

$H_0: {F}_x(t) = {F}_0(t)$ for all t

$H_1: {F}_x(t) \ne {F}_0(t)$ for some t

We have one sample, so we will use Kolmogorov’s Goodness-of-fit test (**one-sample Kolmogorov–Smirnov test)** to compare the empirical CDF of {$x_k$}$^{100}_{k=1}$ with the CDF of normal distribution with parameters calculated from the sample.

So test statistic:

$$
d = \sup_{t}|\hat{F}_x(t)-{F}_0(t)|
$$

where $\hat{F}_x(t)$ is the empirical CDF of {$x_k$}$^{100}_{k=1}$ and ${F}_0(t)$ is CDF of normal distribution with parameters $\mu$ and $\sigma$, which are estimated from the sample.

```{r}
sample_mean <- mean(xk)
sample_sd <- sd(xk)

ks_norm <- ks.test(xk, "pnorm", mean = sample_mean, sd = sample_sd)
ks_norm
```

The **test statistic is 0.06513**. This value is small, which means that the empirical CDF of {$x_k$}$^{100}_{k=1}$ is close to the CDF of the normal distribution $F_0(t)$.

The **p value is 0.7899**. Since it's \> 0.05, we don't reject the null hypothesis $H_0$ at the 0.05 significance level. This means that {$x_k$}$^{100}_{k=1}$ are consistent with the normal distribution.

(b) {$|x_k|$}$^{100}_{k=1}$ are exponentially distributed with $\lambda$ = 1;

We want to check whether {$|x_k|$}$^{100}_{k=1}$ has exponential distribution.

$H_0: {F}_{|x|}(t) = {F}_0(t)$ for all t

$H_1: {F}_{|x|}(t) \ne {F}_0(t)$ for some t

We have one sample, so we will use Kolmogorov’s Goodness-of-fit test (**one-sample Kolmogorov–Smirnov test)** to compare the empirical CDF of {$|x_k|$}$^{100}_{k=1}$ with the CDF of exponential distribution.

So test statistic:

$$
d = \sup_{t}|\hat{F}_{|x|}(t)-{F}_0(t)|
$$

where $\hat{F}_x(t)$ is the empirical CDF of {$|x_k|$}$^{100}_{k=1}$ and ${F}_0(t)$ is CDF of exponential distribution with parameter $\lambda$ = 1.

```{r}
# theoretical E(X) = 1
# theoretical Var(X) = 1
abs_x <- abs(xk)

ks_exp <- ks.test(abs_x, "pexp", rate = 1)
ks_exp
```

The **test statistic is 0.17022**. This value is quite large, meaning that the empirical CDF of {$|x_k|$}$^{100}_{k=1}$ differs significantly from $F_0(t)$ - the theoretical CDF of the exponential distribution with $\lambda$ = 1.

The **p value is 0.006078**. Since it's $\leq$ 0.05, we reject the null hypothesis $H_0$ at the 0.05 significance level. This means that {$|x_k|$}$^{100}_{k=1}$ don't follow exponential distribution with $\lambda$ = 1.

(c) {$x_k$}$^{100}_{k=1}$ and {$y_l$}$^{50}_{l=1}$ have the same distributions.

We want to check whether {$x_k$}$^{100}_{k=1}$ and {$y_l$}$^{50}_{l=1}$ have the same distribution.

$H_0: {F}_x(t) = {F}_y(t)$ for all t

$H_1: {F}_x(t) \ne {F}_y(t)$ for some t

We have two samples, so we will use **two-sample Kolmogorov–Smirnov test** to compare two empirical CDFʼs.

So test statistic:

$$
d = \sup_{t}|\hat{F}_x(t)-\hat{F}_y(t)|
$$

where $\hat{F}_x(t)$ is the empirical CDF of {$x_k$}$^{100}_{k=1}$ and $\hat{F}_y(t)$ is the empirical CDF of {$y_l$}$^{50}_{l=1}$.

```{r}
ks_two_sample <- ks.test(xk, yl)
ks_two_sample
```

The **test statistic is 0.1**, which measures the maximum difference between two empirical CDF's. This value is small, meaning that the empirical CDF of {$x_k$}$^{100}_{k=1}$ is close to the empirical CDF of {$y_l$}$^{50}_{l=1}$.

The **p value is 0.8846**. Since it's $>$ 0.05, we don't reject the null hypothesis $H_0$ at the 0.05 significance level. This means that two samples have very similar distributions.

## Problem 4

In this task you’ll practice fitting the regression line to some real-life features and analyzing the results. The file data.csv contains data on students, specifically their study time and corresponding marks. Your tasks are as follows:

(a) Create a scatter plot of Marks vs. Study Time and provide brief comments;

(b) Fit a linear regression model using marks as the dependent variable and study time as the independent variable. Explain shortly the process of deriving the regression equation;

(c) Evaluate the goodness-of-fit for the fitted line;

(d) Suggest a way to test whether the study time is significant in predicting marks. Find the corresponding test statistics, specify its distribution. Find the p-value of the test and make a conclusion;

(e) If Alice studies for approximately 8 hours, what grade can we predict for her?

(f) Suggest up to three ideas to potentially improve prediction accuracy;

```{r}
data <- read.csv("data.csv")

str(data)
summary(data)
```

## a.

```{r}
plot(
  x = data$time_study,
  y = data$Marks,
  xlab = "Study time",
  ylab = "Marks",
  main = "Marks vs Study time",
  pch = 16
)
```

The scatter plot of Marks vs Study time shows an approximately linear relationship: students who study more tend to receive higher marks. The points are reasonably close to a straight line and there are no extreme outliers, so a simple linear model looks like a good first approximation.

## b.

```{r}
model.lm <- lm(lm(Marks ~ time_study, data = data))

plot(x,y,  
     xlab = "Study time",
    ylab = "Marks",
    main = "Marks vs Study time + linear regression",
    pch = 16, 
    col = "blue"
    ) 
model_summary <- summary(model.lm)

abline(model.lm, col = "red")

```

We fit a simple linear regression model using `lm(Marks ~ time_study)`, where Marks is the dependent variable and study time is the predictor. The `lm` function chooses the intercept and slope that minimise the sum of squared vertical distances between the observed marks and the fitted line. This gives us a regression equation of the form

$$
\widehat{Marks}=\hat\alpha+\hat\beta⋅\text{time_study}+\epsilon,
$$

The coefficients $\hat\alpha$​ (intercept) and $\hat\beta$ (slope) are chosen by the method of ordinary least squares. For each observation $i$ we have an actual mark $y_i$​ and a fitted value $\hat y_i = \hat\alpha + \hat\beta x_i$, where $x_i$​ is the study time. We define the residuals $\epsilon_i = y_i - \hat y_i$ and choose $\hat\alpha,\hat\beta$​ to minimise the sum of squared residuals

$$
S(\alpha,\beta)=\sum_{i=1}^n (y_i - \alpha - \beta x_i)^2.
$$
Taking partial derivatives of $S$​, setting them to zero and solving the resulting “normal equations” gives the explicit formulas

$$
\hat\beta = \frac{\sum_{i=1}^n (x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^n (x_i-\bar x)^2}, 
\qquad
\hat\alpha = \bar y - \hat\beta \bar x,
$$

where $\bar x$ and $\bar y$​ are the sample means of study time and marks. The lm() function in R performs these calculations and returns the estimated parameters.

## c. and d.

```{r}
r2 <- model_summary$r.squared
r2adj <- model_summary$adj.r.squared

coeff <- model_summary$coefficients

t_val <- coeff["time_study", "t value"]
p_val <- coeff["time_study", "Pr(>|t|)"]

cat("R^2:", r2, "\n")
cat("R^2 adjusted:", r2adj, "\n")
cat("t-value:", t_val, "\n")
cat("p-value:", p_val, "\n")
```

The coefficient of determination is $R^2 \approx 0.888$, meaning that about 88.8% of the variability in marks is explained by study time alone. The adjusted $R^2 \approx 0.887$ is almost identical, which indicates means the model is not overfitting and that the linear relationship captures most of the structure in the data. Overall, the fitted line provides a very good fit.

To test whether study time is a significant predictor of marks, we test

$$
H_0 : \beta = 0 \quad \text{vs} \quad H_1 : \beta \neq 0.
$$

In simple linear regression the test statistic for this hypothesis is

$$
t = \frac{\hat\beta}{\text{SE}(\hat\beta)},
$$

which follows a Student t distribution with $n-2$ degrees of freedom under $H_0$​.\

In our model the t-value for `time_study` is about 27.85, with a p-value of roughly $2.36 \times 10^{-48}$. This p-value is essentially zero on any reasonable significance level (e.g. 0.05), so we strongly reject $H_0$​. Study time is highly significant in predicting marks, and the true slope is very unlikely to be 0.

## e.

```{r}
new_data <- data.frame(time_study = 8)

data_pred <- predict(model.lm, newdata = new_data)
data_pred
```

Using the fitted regression line, for a student who studies about 8 hours we obtain a predicted mark of approximately 46.7.

## f.

Some ideas to make model better:

-   *We can add more predictors:* other variables that obviously affect marks (attendance, sleep quality, type of course, etc.), not only study time.

-   *We can allow non-linear effects:* the relationship might flatten out for very large study times, so using a simple transformation could capture this.

-   *We can check and clean the data:* identify and possibly remove clear outliers or data entry errors, and make sure marks and study time are measured consistently. Cleaner data usually means better predictions.
